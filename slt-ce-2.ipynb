{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLT-CE-2: Deterministic Annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "<li> Deterministic annealing for clustering, compression, classification, regression, and related optimization\n",
    "problems, Kenneth Rose, 1998, http://ieeexplore.ieee.org/document/726788/\n",
    "</li>\n",
    "    \n",
    "<li>\n",
    "A Ratio Scale Metric and the Compatibility\n",
    "of Ratio Scales: The Possibility of\n",
    "Arrow‚Äôs Impossibility Theorem, T.L. Saalty, 1994, https://www.sciencedirect.com/science/article/pii/0893965994900930\n",
    "</li>\n",
    "\n",
    "<li>\n",
    "The wine data set, http://www3.dsi.uminho.pt/pcortez/wine5.pdf\n",
    "</li>\n",
    "    \n",
    "<li>\n",
    "Lecture 4, slide 19, https://ml2.inf.ethz.ch/courses/slt/lectures/slt20_lecture04.pdf\n",
    "</li>\n",
    "    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEGI: 11-912-243"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: treelib in /home/paulina/anaconda3/lib/python3.7/site-packages (1.6.1)\r\n",
      "Requirement already satisfied: future in /home/paulina/anaconda3/lib/python3.7/site-packages (from treelib) (0.18.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install treelib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "import sklearn.cluster as cluster\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.svm as svm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Make sure to install treelib in the slt-ce conda environment: conda install treelib\n",
    "import treelib as tl\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "from scipy.linalg import svdvals, eigvalsh, eigvals\n",
    "from math import exp\n",
    "from numpy.linalg import norm\n",
    "\n",
    "#import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#f0b375;\">\n",
    "Section Preliminary\n",
    "<span style=font-size:50%> Complete all problems in this and previous sections to get a grade of 0.0 </span>\n",
    "</h2>\n",
    "\n",
    "<p style=\"background-color:#adebad;\">\n",
    "    Implement the function read_X_y_from_csv according to the contract in its docstring.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_X_y_from_csv(sheet, y_names=None):\n",
    "    \"\"\"Parse a column data store into X, y arrays\n",
    "\n",
    "    Args:\n",
    "        sheet (str): Path to csv data sheet.\n",
    "        y_names (list of str): List of column names used as labels.\n",
    "\n",
    "    Returns:\n",
    "        X (np.ndarray): Array with feature values from columns that are not contained in y_names (n_samples, n_features)\n",
    "        y (dict of np.ndarray): Dictionary with keys y_names, each key contains an array (n_samples, 1)\n",
    "                                with the label data from the corresponding column in sheet. \n",
    "    \"\"\"\n",
    "\n",
    "    # Your code goes here\n",
    "    X = pd.read_csv('wine-data.csv', sep=',')\n",
    "    y = {}\n",
    "    for key in y_names:\n",
    "      y[key] = X[key].tolist()\n",
    "      X = X.drop(columns=key)    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#adebad;\">\n",
    "Read the wine data [3], which contains 11 physiochemical attributes, and two labels (quality and color).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = read_X_y_from_csv(\"wine-data.csv\", y_names=[\"quality\", \"color\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#f0b375;\">\n",
    "Section 4.0\n",
    "<span style=font-size:50%> Complete all problems in this and previous sections to get a grade of 4.0 </span>\n",
    "</h2>\n",
    "\n",
    "<p style=\"background-color:#adebad;\">\n",
    "    Read reference [1] about deterministic annealing clustering (DAC). Shortly summarize what they refer to as the <i>preferred implementation</i> of the DAC algorithm.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-384dc749b9cf>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-384dc749b9cf>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    The preferred implementation refers to the mass-constrained clustering. The motivation of doing this is to eliminate the annealing process's dependency on the number of coincident codevectors in each effective cluster. The algorithm starts with only one cluster ùë¶0\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "The preferred implementation refers to the mass-constrained clustering. The motivation of doing this is to eliminate the annealing process's dependency on the number of coincident codevectors in each effective cluster. The algorithm starts with only one cluster ùë¶0\n",
    ", the mass center of the dataset. The initial temperature should be larger than 2 times the biggest eigenvalue of the convariance matrix of the posterior distribution ùëù(ùë•|ùë¶0). For each cluster ùë¶ùëñ, we first calculate the ùëù(ùë¶ùëñ|ùë•) and ùëù(ùë¶ùëñ), and then calculate ùë¶ùëñ based on ùëù(ùë¶ùëñ|ùë•) and ùëù(ùë¶ùëñ). The formula of ùë¶ùëñ is derived by optimizing the cost function ùêπ‚Ä≤=ùê∑‚àíùëáùêª+ùúÜ(‚àëùëñùëùùëñ‚àí1), where ùê∑ is the distortion, ùëá is the temperature and ùêª is the entropy of ùëå|ùëã. We keep updating the centroids until the current temperature until they converge. We lower the temperature according to the cooling schedule. Later on, we check for each cluster if it is about to split by comparing the critical temperature with the current temperature. When the temperature is lower than the pre-set lowest temperature, we set ùëá=0 and do the last round update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your markdown text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#adebad;\">\n",
    "    Implement the <b>fit method</b> for the template class DeterministicAnnealing, according to the contract outlined in its docstring.\n",
    "    You can add more class methods as necessary.\n",
    "    See http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html for complementary information.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicAnnealingClustering(skl.base.BaseEstimator, skl.base.TransformerMixin):\n",
    "    \"\"\"Template class for DAC\n",
    "    \n",
    "    Attributes:\n",
    "        cluster_centers_ (np.ndarray): Cluster centroids y_i (n_clusters, n_features)\n",
    "        cluster_probabs_ (np.ndarray): Assignment probability vectors p(y_i | x) for each sample\n",
    "                                       (n_samples, n_clusters)\n",
    "        bifurcation_tree_ (treelib.Tree): Tree object that contains information about cluster evolution during\n",
    "                                          annealing.\n",
    "                                       \n",
    "    Parameters:\n",
    "        n_clusters (int): Maximum number of clusters returned by DAC.\n",
    "        random_state (int): Random seed.\n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, n_clusters=8, random_state=42, metric=\"euclidian\", bt=False):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.random_state = random_state\n",
    "        self.metric = metric\n",
    "        self.bt = bt\n",
    "        # Add more parameters, if necessary.\n",
    "        \n",
    "    def association_prob(self, x, Y, i, py, T, K, scalar):\n",
    "        \"\"\"return a scalar corresponding to p(y_j|x_i)\"\"\"\n",
    "        diff = x - Y\n",
    "        distortion = np.linalg.norm(diff, axis=1)**2\n",
    "        q = -distortion/T\n",
    "        \n",
    "        exp_arr = np.exp(q-q.max()) # exp-normalize to avoid overflow: https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "        denominator = np.dot(py, exp_arr)\n",
    "        if scalar:\n",
    "          ap = (py[K-1]*exp_arr)/denominator\n",
    "        else:\n",
    "          ap = (py[K-1]*exp_arr[K-1])/denominator\n",
    "        return ap\n",
    "        \n",
    "    def association_prob_arr(self, X, Y, py, K, k, T):\n",
    "        \"\"\"For a fixed center and all samples\n",
    "        Return:\n",
    "          apa: nd-array (samples, 1)\n",
    "        \"\"\"\n",
    "        samples = X.shape[0]\n",
    "        apa = np.zeros(samples)\n",
    "        Y = Y[0:K]\n",
    "        py = py[0:K]\n",
    "        \n",
    "        for i in range(samples):\n",
    "          apa[i] = self.association_prob(X[i], Y, i, py, T, k, K == 1)\n",
    "          \n",
    "        return apa\n",
    "\n",
    "    def compute_distortion(self, X, Y, K, ap_mat):\n",
    "      distances = euclidean_distances(X, Y[0:K])\n",
    "      weighted_distances = distances.T*ap_mat[0:K]\n",
    "      return weighted_distances.sum()\n",
    "      \n",
    "    def ratio_scale(self, x, y, d):\n",
    "      scale = 1/(d*d)\n",
    "      scale = tf.constant(scale, dtype=tf.float64)\n",
    "      s = tf.Variable(0, dtype = tf.float64)\n",
    "      for i in range(d):\n",
    "        for j in range(d):\n",
    "          s = s+(x[i]*y[j])/(x[j]*y[i])\n",
    "      \n",
    "      return tf.log(scale*s)\n",
    "    \n",
    "    def ratio_scale_cost(self, X, Y, T):\n",
    "      cost = tf.Variable(0, dtype=tf.float64)\n",
    "      # print(X.shape, Y.shape)\n",
    "      samples, features, clusters = X.shape[0], int(X.shape[1]), Y.shape[0]\n",
    "      for i in range(samples):\n",
    "        subcost = tf.Variable(0,dtype=tf.float64)\n",
    "        for j in range(clusters):\n",
    "          subcost = subcost+tf.exp(-self.ratio_scale(X[i], Y[j], features))/T\n",
    "        cost = cost + tf.log(subcost)\n",
    "      \n",
    "      return -tf.consant(T)*cost/tf.constant(samples)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Compute DAC for input vectors X\n",
    "        \n",
    "        Preferred implementation of DAC as described in reference [1].\n",
    "        Consider to use initialization and reseeding as in sklearn k-means for improved performance.\n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray): Input array with shape (samples, n_features)\n",
    "        \n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        X = X.values\n",
    "        samples, features = X.shape\n",
    "        K_max = self.n_clusters\n",
    "        metric = self.metric\n",
    "                \n",
    "        T_min = 0.001\n",
    "        eta = 0.2\n",
    "        \n",
    "        K = 1\n",
    "        y0 = np.mean(X, axis=0) # The very first center\n",
    "        \n",
    "        Y = np.zeros((K_max, features))\n",
    "        Y[0] = y0\n",
    "        \n",
    "        \n",
    "        \n",
    "        py = np.zeros(K_max) # p(y_i)\n",
    "        py[0] = 1\n",
    "        \n",
    "        la_max = eigvalsh(np.cov(X.T), check_finite = False)[-1]\n",
    "        T  = 2*la_max + 1.0\n",
    "        \n",
    "        if self.bt:\n",
    "          # Write to the tree\n",
    "          cnt = 0\n",
    "          self.beta = []\n",
    "          self.bifurcation_tree_ = tl.Tree()\n",
    "          id_hash_table = {} # map the cluster number to node id\n",
    "          id_hash_table[0] = 0\n",
    "          self.bifurcation_tree_.create_node(identifier=0, data={\n",
    "              \"cluster\": 0,\n",
    "              \"ref\": y0,\n",
    "              \"offset\": 0,\n",
    "              \"from\": cnt,\n",
    "              \"to\": -1,\n",
    "              \"distance\": [],\n",
    "              \"direction\": 1\n",
    "          })\n",
    "\n",
    "        ap_mat = np.zeros((K_max, samples)) # Association probability matrix\n",
    "        \n",
    "        if metric == \"euclidian\":\n",
    "            while T > T_min:\n",
    "              if self.bt:\n",
    "                self.beta.append(1/T)\n",
    "              Y_old = np.copy(Y) # centroids from last turn\n",
    "              for i in range(K):\n",
    "                apa = self.association_prob_arr(X, Y, py, K, i+1, T)\n",
    "                ap_mat[i] = apa\n",
    "                py[i] = apa.sum()\n",
    "                nominator = apa@X\n",
    "                Y[i] = nominator/py[i]\n",
    "              \n",
    "              if not np.allclose(Y, Y_old): # convergence test\n",
    "                continue\n",
    "              \n",
    "              if self.bt:\n",
    "                # Update the tree\n",
    "                cnt += 1\n",
    "                for node in self.bifurcation_tree_.expand_tree():\n",
    "                  data = self.bifurcation_tree_[node].data\n",
    "                  if data[\"to\"] != -1: # it has splitted, leave it alone\n",
    "                    continue\n",
    "                  dist = np.linalg.norm(Y[data[\"cluster\"]] - data[\"ref\"])*data[\"direction\"] + data[\"offset\"]\n",
    "                  print(\"id \", node)\n",
    "                  data[\"distance\"].append(dist)\n",
    "\n",
    "                self.beta.append(1/T)\n",
    "              \n",
    "              T *= eta # cooling\n",
    "              \n",
    "              if K < K_max:\n",
    "                # check splitting\n",
    "                for i in range(K):\n",
    "                  if K >= K_max:\n",
    "                    continue\n",
    "                  y = Y[i]\n",
    "                  coefficient = ap_mat[i]/(py[i]*samples) # p(x) = 1/N\n",
    "                  Cxy = np.zeros((features, features))\n",
    "                  for j in range(samples):\n",
    "                    Cxy += coefficient[j] * np.outer(X[j]-y, X[j]-y)\n",
    "                   \n",
    "                  critical = 2*max(eigvals(Cxy, check_finite=False))\n",
    "                  \n",
    "                  if T < critical:\n",
    "                    Y[K] = Y[i]+np.random.normal(0, 1, size=features)\n",
    "                    py[i] /= 2\n",
    "                    py[K] = py[i]\n",
    "                    if self.bt:\n",
    "                      # Create new nodes\n",
    "                      offset = self.bifurcation_tree_.get_node(id_hash_table[i]).data[\"distance\"][-1]\n",
    "                      print(\"offsert = \",offset)\n",
    "                      idd = id_hash_table[i]\n",
    "                      self.bifurcation_tree_.create_node(identifier = idd*13+1, parent=id_hash_table[i], data={\n",
    "                          \"ref\": Y[i], # deep copy\n",
    "                          \"cluster\": i,\n",
    "                          \"offset\": offset,\n",
    "                          \"from\": cnt-1,\n",
    "                          \"to\": -1,\n",
    "                          \"direction\": -1,\n",
    "                          \"distance\": [offset]\n",
    "                      }) # continuation of the original branch\n",
    "                      \n",
    "                      id_hash_table[i] = idd*13+1\n",
    "                      self.bifurcation_tree_.create_node(identifier = idd*13+3, parent=id_hash_table[i], data={\n",
    "                          \"ref\": Y[i], # deep copy\n",
    "                          \"cluster\": K,\n",
    "                          \"offset\": offset,\n",
    "                          \"from\": cnt-1,\n",
    "                          \"to\": -1,\n",
    "                          \"direction\": 1,\n",
    "                          \"distance\": [offset]\n",
    "                      }) # the new branch\n",
    "                      id_hash_table[K] = idd*13+3\n",
    "                      self.bifurcation_tree_.get_node(idd).data[\"to\"] = cnt\n",
    "                    K += 1\n",
    "            \n",
    "            if self.bt:\n",
    "              self.beta.append(T)\n",
    "              for node in self.bifurcation_tree_.expand_tree():\n",
    "                  data = self.bifurcation_tree_[node].data\n",
    "                  if data[\"to\"] == -1: # it has splitted, leave it alone\n",
    "                    self.bifurcation_tree_[node].data[\"to\"] = cnt\n",
    "            \n",
    "            self.prob_y = py\n",
    "            ap_mat = (ap_mat >= np.max(ap_mat, axis=0)).astype(np.int_)\n",
    "            \n",
    "            # When T is smaller than T_min\n",
    "            for i in range(K):\n",
    "              apa = ap_mat[i]\n",
    "              py[i] = apa.sum()\n",
    "              nominator = apa@X\n",
    "              Y[i] = nominator/py[i]\n",
    "       \n",
    "      \n",
    "      \n",
    "      \n",
    "        elif metric == \"ratioscale\":     \n",
    "            X_tf = tf.constant(X.astype('float64'))\n",
    "            while T > T_min:\n",
    "              # Use gradient descend to get Y\n",
    "              Y = np.ones((K, features))\n",
    "              Y = tf.Variable(Y)\n",
    "              # print(Y.shape)\n",
    "              cost_function = self.ratio_scale_cost(X_tf, Y, T)\n",
    "              print(\"hi\")\n",
    "              GDO = tf.train.GradientDescendOptimizer(learning_rate = 0.001)\n",
    "              GDO.minimize(cost_function)\n",
    "              T *= eta # cooling\n",
    "              \n",
    "              if K < K_max:\n",
    "                # check splitting\n",
    "                for i in range(K):\n",
    "                  if K >= K_max:\n",
    "                    continue\n",
    "                  y = Y[i]\n",
    "                  coefficient = ap_mat[i]/(py[i]*samples) # p(x) = 1/N\n",
    "                  Cxy = np.zeros((features, features))\n",
    "                  for j in range(samples):\n",
    "                    Cxy += coefficient[j] * np.outer(X[j]-y, X[j]-y)\n",
    "                   \n",
    "                  critical = 2*max(eigvals(Cxy, check_finite=False))\n",
    "                  \n",
    "                  if T < critical:\n",
    "                    Y[K] = Y[i]+np.random.normal(0, 1, size=features)\n",
    "                    py[i] /= 2\n",
    "                    py[K] = py[i]\n",
    "                    K += 1\n",
    "            \n",
    "            self.prob_y = py\n",
    "            ap_mat = (ap_mat >= np.max(ap_mat, axis=0)).astype(np.int_)\n",
    "            \n",
    "            # When T is smaller than T_min\n",
    "            for i in range(K):\n",
    "              apa = ap_mat[i]\n",
    "              py[i] = apa.sum()\n",
    "              nominator = apa@X\n",
    "              Y[i] = nominator/py[i]\n",
    "        \n",
    "        \n",
    "        self.cluster_centers = Y\n",
    "        self.cluster_probabs = ap_mat\n",
    "        \n",
    "        \n",
    "        return self\n",
    "      \n",
    "    def fit_phase_transition(self, X):\n",
    "        \"\"\"Compute DAC for input vectors X\n",
    "        \n",
    "        Preferred implementation of DAC as described in reference [1].\n",
    "        Consider to use initialization and reseeding as in sklearn k-means for improved performance.\n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray): Input array with shape (samples, n_features)\n",
    "        \n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        X = X.values\n",
    "        samples, features = X.shape\n",
    "        K_max = self.n_clusters\n",
    "        metric = self.metric\n",
    "                \n",
    "        T_min = 0.001\n",
    "        eta = 0.18\n",
    "        \n",
    "        K = 1\n",
    "        y0 = np.mean(X, axis=0) # The very first center\n",
    "        \n",
    "        Y = np.zeros((K_max, features))\n",
    "        Y[0] = y0\n",
    "        \n",
    "        \n",
    "        \n",
    "        py = np.zeros(K_max) # p(y_i)\n",
    "        py[0] = 1\n",
    "        \n",
    "        la_max = eigvalsh(np.cov(X.T), check_finite = False)[-1]\n",
    "        T  = 2*la_max + 1.0\n",
    "        \n",
    "        self.distortion = [] # record each distortion at each temperature\n",
    "        self.eff_clusters = [] # record each distortion at each temperature\n",
    "        self.beta = []\n",
    "        self.emergence = []\n",
    "        cnt = 0\n",
    "\n",
    "        ap_mat = np.zeros((K_max, samples)) #probability matrix\n",
    "        \n",
    "        if metric == \"euclidian\":\n",
    "            while T > T_min:\n",
    "              Y_old = np.copy(Y) # centroids from last turn\n",
    "              for i in range(K):\n",
    "                apa = self.association_prob_arr(X, Y, py, K, i+1, T)\n",
    "                ap_mat[i] = apa\n",
    "                py[i] = apa.sum()\n",
    "                nominator = apa@X\n",
    "                Y[i] = nominator/py[i]\n",
    "              \n",
    "              if not np.allclose(Y, Y_old): # convergence test\n",
    "                continue\n",
    "              \n",
    "              self.distortion.append(self.compute_distortion(X, Y, K,ap_mat)) # append the distortion corresponding to the current temperature\n",
    "              self.beta.append(1/T) # append the current temperature\n",
    "              self.eff_clusters.append(K)\n",
    "              \n",
    "              T *= eta # temperature down\n",
    "              \n",
    "              K_old = K\n",
    "              if K < K_max:\n",
    "                # check splitting\n",
    "                for i in range(K):\n",
    "                  if K >= K_max:\n",
    "                    continue\n",
    "                  y = Y[i]\n",
    "                  coefficient = ap_mat[i]/(py[i]*samples) # p(x) = 1/N\n",
    "                  Cxy = np.zeros((features, features))\n",
    "                  for j in range(samples):\n",
    "                    Cxy += coefficient[j] * np.outer(X[j]-y, X[j]-y)\n",
    "                   \n",
    "                  critical = 2*max(eigvals(Cxy, check_finite=False))\n",
    "                  \n",
    "                  if T < critical:\n",
    "                    Y[K] = Y[i]+np.random.normal(0, 1, size=features)\n",
    "                    py[i] /= 2\n",
    "                    py[K] = py[i]\n",
    "                    K += 1\n",
    "              \n",
    "              if K_old < K:\n",
    "                self.emergence.append(cnt)\n",
    "            \n",
    "              cnt += 1\n",
    "\n",
    "            self.prob_y = py\n",
    "            ap_mat = (ap_mat >= np.max(ap_mat, axis=0)).astype(np.int_)\n",
    "            \n",
    "            # When T is smaller than T_min\n",
    "            for i in range(K):\n",
    "              apa = ap_mat[i]\n",
    "              py[i] = apa.sum()\n",
    "              nominator = apa@X\n",
    "              Y[i] = nominator/py[i]\n",
    "        \n",
    "        elif metric == \"ratioscale\":\n",
    "            \n",
    "            # code for extension\n",
    "            pass\n",
    "            \n",
    "        self.cluster_centers = Y\n",
    "        self.cluster_probabs = ap_mat\n",
    "        \n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict assignment probability vectors for each sample in X.\n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray): Input array with shape (new_samples, n_features)\n",
    "            \n",
    "        Returns:\n",
    "            P (np.ndarray): Assignment probability vectors (new_samples, n_clusters) \n",
    "        \"\"\"\n",
    "        X = X.values\n",
    "        samples, features = X.shape\n",
    "        ap_mat = np.zeros((self.n_clusters, samples))\n",
    "        \n",
    "        for i in range(self.n_clusters):\n",
    "          apa = self.association_prob_arr(X, self.cluster_centers, self.prob_y, self.n_clusters, i+1, T=10)\n",
    "          ap_mat[i] = apa\n",
    "        \n",
    "        return ap_mat.T\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X to a cluster-distance space.\n",
    "        \n",
    "        In the new space, each dimension is the distance to the cluster centers. \n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray): Input array with shape (new_samples, n_features)\n",
    "            \n",
    "        Returns:\n",
    "            Y (np.ndarray): Cluster-distance vectors (new_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, [\"cluster_centers\"])\n",
    "        X = X.values\n",
    "        Y = euclidean_distances(X, self.cluster_centers)\n",
    "        \n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def plot_bifurcation(self):\n",
    "        \"\"\"Show the evolution of cluster splitting\"\"\"\n",
    "        check_is_fitted(self, [\"bifurcation_tree_\"])\n",
    "        \n",
    "        tree = self.bifurcation_tree_\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_ylim(0,0.03)\n",
    "        for node in tree.expand_tree():\n",
    "          x = tree[node].data[\"distance\"]\n",
    "          f = tree[node].data[\"from\"]\n",
    "          t = tree[node].data[\"to\"]\n",
    "          beta = self.beta[f:t]\n",
    "          ax.plot(x, beta)\n",
    "        \n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#adebad;\">\n",
    "    Create an instance of your DAC class with n_clusters = 2 and <b>fit the first 6000 samples</b> of the wine data set. Record the execution time. Furthermore, create an instance of the sklearn k-means class, and fit it with the same parameters. Again record the execution time.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X,\n",
    "                                                                        y[\"color\"],\n",
    "                                                                        train_size=6000,\n",
    "                                                                        random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21 s, sys: 288 ms, total: 21.3 s\n",
      "Wall time: 5.31 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeterministicAnnealingClustering(bt=False, metric='euclidian', n_clusters=2,\n",
       "                                 random_state=42)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "DAC = DeterministicAnnealingClustering(n_clusters=2, random_state=42)\n",
    "DAC.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 284 ms, sys: 0 ns, total: 284 ms\n",
      "Wall time: 71 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=2, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "kmeans = cluster.KMeans(n_clusters=2,random_state=42)\n",
    "kmeans.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#f0b375;\">\n",
    "Section 4.5\n",
    "<span style=font-size:50%> Complete all problems in this and previous sections to get a grade of 4.5 </span>\n",
    "</h2>\n",
    "\n",
    "<p style=\"background-color:#adebad;\">\n",
    "    <ul style=\"background-color:#adebad;\">\n",
    "        <li> \n",
    "            Implement the <b>predict method</b> for the template class DAC, according to the contract outlined in its docstring.\n",
    "        </li>\n",
    "        <li>\n",
    "            Use DAC.predict and kmeans.predict to predict the cluster labels of X_test.\n",
    "        </li>\n",
    "        <li>\n",
    "            Compute the confusion matrix between the two predictions as described in <br>\n",
    "            http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "        </li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.31 ms, sys: 100 ¬µs, total: 5.41 ms\n",
      "Wall time: 1.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_kmeans = kmeans.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 97.6 ms, sys: 12.2 ms, total: 110 ms\n",
      "Wall time: 25.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_DAC = DAC.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#adebad;\">\n",
    "<li> Before we can compute the confusion matrix, we need to perform some post-processing on the DAC cluster assignments.\n",
    "    Explain what the function postprocess (defined below) does, and why we need it. To do so, complete the docstring of the function postprocess.\n",
    "        </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(y_DAC, y_kmeans):\n",
    "    \"\"\"The purpose of this function is to make\n",
    "    the results from two methods comparable. Because the indices of \n",
    "    clusters may be different in two methods, we have to caliberate the DAC's\n",
    "    cluster assignments according to kmeans'. \n",
    "    Args:\n",
    "            y_DAC (np.ndarray): (fuzzy) Clustering assignment by DAC (samples, n_features)\n",
    "            y_kmeans (np.ndarray): Clustering assignment by kmeans (samples, )\n",
    "        \n",
    "    Returns:\n",
    "            y_DAC_new (np.ndarray): Caliberated clustering assignment by DAC (samples, )\n",
    "    \"\"\"\n",
    "    \n",
    "    y_DAC_hard = np.argmax(y_DAC, axis=1)\n",
    "    \n",
    "    n_clusters = len(np.unique(y_DAC_hard))\n",
    "    dac2kmeans = []\n",
    "    for cluster in range(n_clusters):\n",
    "        argmax = np.argmax(y_DAC[:, cluster])\n",
    "        dac2kmeans.append(y_kmeans[argmax])\n",
    "        \n",
    "    y_DAC_new = []\n",
    "    for dac_label in y_DAC_hard:\n",
    "        y_DAC_new.append(dac2kmeans[dac_label])\n",
    "        \n",
    "    return np.array(y_DAC_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[287   1   0 209]\n"
     ]
    }
   ],
   "source": [
    "cm = skl.metrics.confusion_matrix(y_kmeans, postprocess(y_DAC, y_kmeans))\n",
    "print(cm.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#f0b375;\">\n",
    "Section 5.0\n",
    "<span style=font-size:50%> Complete all problems in this and previous sections to get a grade of 5.0 </span>\n",
    "</h2>\n",
    "\n",
    "<ul style=\"background-color:#adebad;\">\n",
    "        <li> Implement the <b>transform method</b> for the template class DAC, according to the contract outlined in its docstring.\n",
    "        </li>\n",
    "        <li>\n",
    "        Use DAC.transform and kmeans.transform to transform both, X_train and X_test. \n",
    "        </li>\n",
    "       \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train_DAC = DAC.transform(X_train)\n",
    "X_test_DAC = DAC.transform(X_test)\n",
    "\n",
    "X_train_kmeans = kmeans.transform(X_train)\n",
    "X_test_kmeans = kmeans.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#adebad;\">\n",
    "        <li>\n",
    "        Fit an SVM classifier with default parameters to the untransformed data, and to the transformed data.\n",
    "        Compare the performance of predicting whether the color of a wine is red or white.\n",
    "        </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9275653923541247"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svm = svm.SVC(random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9134808853118712"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svm_DAC = svm.SVC(random_state=42)\n",
    "svm_DAC.fit(X_train_DAC, y_train)\n",
    "svm_DAC.score(X_test_DAC, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9134808853118712"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svm = svm.SVC(random_state=42)\n",
    "svm.fit(X_train_kmeans, y_train)\n",
    "svm.score(X_test_kmeans, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#adebad;\">\n",
    "        <li>\n",
    "        Produce two scatter plots, one for X_train_DAC and one for X_train_kmeans.<br>\n",
    "        Make the marker color indicate the wine color.\n",
    "        </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_idx = []\n",
    "red_idx = []\n",
    "for cnt, color in enumerate(y_train):\n",
    "  if color == \"red\":\n",
    "    red_idx.append(cnt)\n",
    "  else:\n",
    "    white_idx.append(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f9560131c50>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(121)\n",
    "plt.scatter(X_train_DAC[red_idx, 0], X_train_DAC[red_idx,1], c='red')\n",
    "plt.scatter(X_train_DAC[white_idx, 0], X_train_DAC[white_idx,1], c='blue')\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_train_kmeans[red_idx, 0], X_train_kmeans[red_idx,1], c='red')\n",
    "plt.scatter(X_train_kmeans[white_idx, 0], X_train_kmeans[white_idx,1], c='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#adebad;\">\n",
    "    <li>\n",
    "        Create a fixed 2D embedding (e.g. with LLE, t-SNE, MDS) of the wine data and color the markers according to quality and color. Fit and transform X_train with DAC(n_clusters=3,4,5,6,7,8,...). Produce a plot of the SVM score svm_DAC.score(X_test_DAC, y_test) as a function of n_clusters.. Each time use marker shapes to display the cluster memberships, and compare to the labels color and quality.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    %%time\n",
    "    lle = skl.manifold.LocallyLinearEmbedding(random_state=...)\n",
    "    lle.fit(...)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#f0b375;\">\n",
    "Section 5.5\n",
    "<span style=font-size:50%> Complete all problems in this and previous sections to get a grade of 5.5 </span>\n",
    "</h2>\n",
    "<ul style=\"background-color:#adebad;\">\n",
    "        <li>\n",
    "            Produce a phase diagram plot of the expected distortion D, as shown in figure 2 of reference [1]. For this, extend DAC.fit to save the expected distortion during annealing as an additional attribute self.distortion.\n",
    "            You might also want to save the number of effective clusters and the temperature along the way.\n",
    "        </li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#adebad;\">\n",
    "        <li>\n",
    "        Implement DAC.plot_bifurcation, which should create a bifurcation plot as shown on slide 19 of lecture 3. As our data is not 1-dimensional as in the lecture slide, we will have to adapt our scheme, so that the distances between nodes of the tree make sense.<br>\n",
    "        Modify DAC.fit to keep track of the distances, using the tree object DAC.bifurcation\\_tree\\_. When a cluster splits, it creates two child nodes. Each node should store its centroid vector, and the distance to the parent centroid vector. After splitting, the parent node is not updated anymore.<br>\n",
    "        In the bifurcation plot, the horizontal distance of a child node to its parent node should be exactly the distance to the parent centroid vector. The two child nodes should move in opposite directions, i.e. one to the left of the parent and one to the right.\n",
    "        </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#adebad;\">\n",
    "        <li>\n",
    "        Argue how reasonable our method of plotting the bifurcation is. Explain how the 1D-distances between nodes (i.e. nodes that are not siblings) do not correspond exactly to the distances between centroids. Suggest ideas for improvement.\n",
    "        </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#f0b375;\">\n",
    "Section 6.0\n",
    "<span style=font-size:50%> Complete all problems in this and previous sections to get a grade of 6.0 </span>\n",
    "</h2>\n",
    "\n",
    "<ul style=\"background-color:#adebad;\">\n",
    "        <li>\n",
    "            So far, our implementation of DAC assumed that our data is compatible with the euclidian metric. Argue why this assumption is not justified for the wine-data.\n",
    "        </li>\n",
    "    </ul>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#adebad;\">\n",
    "    <li>\n",
    "        All the features of the wine-data set are measured on a ratio scale, which is incompatible with the euclidian metric (Remark: this is not the complete answer to problem 6, argue why they are not compatible). A more appropriate distance is proposed in reference [2]:\n",
    "            <br><br>\n",
    "            $d(x,y)=\\log{ \\frac{1}{d^2} \\sum_{i,j=1}^d \\frac{x_i}{x_j} \\frac{y_j}{y_i}}$\n",
    "            <br><br>\n",
    "            Extend DAC.fit to the case of metric == ratioscale, using d(x,y) as given above.<br>\n",
    "            Hint 1: As this distance does not give a closed form update formula for the centroids $y$, you will need to do gradient descent to update the centroids. You can either calculate the gradient by hand, or use an automatic differentiation tool like Tensorflow. If you calculate the gradient by hand, provide the formula in Latex below.\n",
    "                    <br>\n",
    "            Hint 2: Keep in mind the possibility of negative definite matrices and appropriate regularization when solving for the critical temperature of the ratio scale approach.\n",
    "        </li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#adebad;\">\n",
    "    <li>\n",
    "    Perform experiments to compare the euclidian and ratioscale metrics.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#4286f4;\"> Comments </h2>\n",
    "\n",
    "Let us know what you liked about this exercise, and what we can improve!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
